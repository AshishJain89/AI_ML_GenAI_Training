{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tJUrYrWJMGk"
   },
   "source": [
    "# Objective\n",
    "\n",
    "To present an overview of the four important Agentic AI patterns - Reflection, Tool Use, Planning and Multi-Agent Collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqOSkUT_JW9e"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njha_OQ7KBJY"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5218,
     "status": "ok",
     "timestamp": 1753876167693,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "ewKugwEzKhvf"
   },
   "outputs": [],
   "source": [
    "%pip install -q groq openai openai-swarm langchain langchain-openai langchain-groq langchain-experimental ipykernel\n",
    "# !pip install -q openai==1.55.3 langchain==0.3.7 langchain-openai==0.2.9 langchain-experimental==0.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade groq langchain langchain-groq langchain-experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-SNRUwEJ-Cy"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2082,
     "status": "ok",
     "timestamp": 1753876563424,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "JDKMG7kUKAob"
   },
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "# from langchain_openai import AzureChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from swarm import Swarm, Agent\n",
    "from langchain import hub\n",
    "from groq import Groq\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.agents import create_react_agent, Tool, AgentExecutor\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  #, ToolMessage, AIMessage\n",
    "\n",
    "\n",
    "MODEL_NAME = 'gemma-9b-it'  # 'qwen/qwen3-32b' 'openai/gpt-oss-20b' \n",
    "groq_api_key = os.environ.get('GROQ_API_KEY')\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9KvgfrILsIF"
   },
   "outputs": [],
   "source": [
    "# with open('config-azure.json') as f:\n",
    "#     configs = f.read()\n",
    "\n",
    "# creds = json.loads(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Purana Tareeka\n",
    "client_groq = Groq(api_key=groq_api_key)\n",
    "# Method 2: Naya Tareeka\n",
    "client_groq_2 = ChatGroq(api_key=groq_api_key, model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm_client_groq = Swarm(client_groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDEZZuKtK5wn"
   },
   "outputs": [],
   "source": [
    "# client = AzureOpenAI(api_key=creds['AZURE_OPENAI_KEY'], azure_endpoint=creds['AZURE_OPENAI_ENDPOINT'], api_version='2024-02-01')\n",
    "\n",
    "# llm = AzureChatOpenAI(azure_endpoint=creds['AZURE_OPENAI_ENDPOINT'], api_key=creds['AZURE_OPENAI_KEY'], api_version=\"2024-02-01\", model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# llm = AzureChatOpenAI(azure_endpoint='https://azuse-mdpfnp0w-swedencentral.openai.azure.com/openai/deployments/gpt-4o-2/chat/completions?api-version=2025-01-01-preview', api_key=groq_api_key, api_version=\"2025-01-01-preview\", model=\"gpt-4o-2\", temperature=0)\n",
    "\n",
    "# swarm_client = Swarm(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRw2xxGhMO83"
   },
   "source": [
    "# Pattern 1: Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSICCQyKQWyy"
   },
   "source": [
    "Self-reflection - Ask the LLM to reflect on its own work to improve its answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hqYxgryldDq"
   },
   "source": [
    "Consider the following use case where the LLM is tasked to extract structured information from medical notes. However, instead of asking the LLM to directly provide the answer, we present the output from the generator LLM to a reflector LLM (in this case the same model). The feedback from the reflector is used by the generator to improve its answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1753876997081,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "Q_64XHSNQQvY"
   },
   "outputs": [],
   "source": [
    "MAIN_MODEL = CRITIQUE_MODEL = 'gemma2-9b-it'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system message for the generator is below.\n",
    "\n",
    "Notice how the system message here explicitly acknowledges that feedback might be provided and should be used to improve the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1753877004703,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "iXWcLGKiNg07"
   },
   "outputs": [],
   "source": [
    "medical_note_data = \"\"\"\n",
    "Medical Notes:\n",
    "---\n",
    "Patient Name: Ms. Krishnaveni\n",
    "Age: 45 years\n",
    "Gender: Female\n",
    "\n",
    "Chief Complaint:\n",
    "Ms. Krishnaveni presented with complaints of persistent abdominal pain, bloating, and changes in bowel habits over the past two months.\n",
    "\n",
    "History of Present Illness:\n",
    "Ms. Krishnaveni reports experiencing intermittent abdominal pain, predominantly in the lower abdomen, accompanied by bloating and alternating episodes of diarrhea and constipation. She describes the pain as crampy in nature, relieved partially by defecation but worsening after meals. There is no association with specific food items. She denies any rectal bleeding, unintended weight loss, or fever.\n",
    "\n",
    "Past Medical History:\n",
    "Ms. Krishnaveni has a history of irritable bowel syndrome (IBS), diagnosed five years ago, managed with dietary modifications and occasional use of over-the-counter antispasmodics.\n",
    "\n",
    "Medications:\n",
    "She occasionally takes over-the-counter antispasmodics for symptomatic relief of abdominal discomfort related to IBS.\n",
    "\n",
    "Family History:\n",
    "There is no significant family history of gastrointestinal disorders or malignancies.\n",
    "\n",
    "Social History:\n",
    "Ms. Krishnaveni is a non-smoker and does not consume alcohol. She works as a teacher in a local school.\n",
    "\"\"\"\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are an expert assistant to a hospital administration team working on extracting important information from medical notes made by doctors.\n",
    "Extract relevant information from the note presented by the user with the following schema.\n",
    "- age: integer, age of the patient\n",
    "- gender: string, can be one of male, female or other\n",
    "- diagnosis: string, can be one of migraine, diabetes, arthritis and acne\n",
    "- weight: integer, weight of the patient\n",
    "- smoking: string, can be one of yes or no\n",
    "Use information ONLY from the medical note to come up with the JSON output.\n",
    "\n",
    "If you receive feedback from the user, use it to provide a revised version of your answer.\n",
    "\"\"\"\n",
    "\n",
    "reflection_system_message = \"\"\"\n",
    "You are an expert assistant to a hospital administration team who is tasked to generate critique and recommendations for output from an LLM.\n",
    "The input will contain an attempt by an LLM to extract relevant information in a JSON format of a medical note presented further below\n",
    "The LLM was instructed that the JSON output needs to be extracted according to the following schema.\n",
    "- age: integer, age of the patient\n",
    "- gender: string, can be one of male, female or other\n",
    "- diagnosis: string, can be one of migraine, diabetes, arthritis and acne\n",
    "- weight: integer, weight of the patient\n",
    "- smoking: string, can be one of yes or no\n",
    "\n",
    "When you review the LLM attempt ensure that your critique is in accordance with the above schema.\n",
    "While you are checking the input entered by the user, check if the input contains only the JSON and no additional information.\n",
    "Provide explicit feedback if you notice additional information apart from the JSON.\n",
    "Do not provide any suggestions for the output; restrict yourself to feedback.\n",
    "---\n",
    "{medical_note_data}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaNi8LJpmdqC"
   },
   "source": [
    "Now let us run the first generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1180,
     "status": "ok",
     "timestamp": 1753877127566,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "CLsI6vybPIvu"
   },
   "outputs": [],
   "source": [
    "first_response = client_groq.chat.completions.create(\n",
    "    model=MAIN_MODEL,\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': medical_note_data}\n",
    "    ],\n",
    "    temperature=0.2\n",
    ").choices[0].message.content\n",
    "\n",
    "print(first_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2x75ZAVmkFy"
   },
   "source": [
    "We will now present this output to the reflector that will present a critique according to our instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2190,
     "status": "ok",
     "timestamp": 1753877159542,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "R5G7LoIVPy7L"
   },
   "outputs": [],
   "source": [
    "first_critique = client_groq.chat.completions.create(\n",
    "    model=CRITIQUE_MODEL,\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': reflection_system_message},\n",
    "        {'role': 'user', 'content': first_response}\n",
    "    ],\n",
    "    temperature=0\n",
    ").choices[0].message.content\n",
    "\n",
    "print(first_critique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgyC9rRumuTz"
   },
   "source": [
    "As can be seen from the above output, the reflector identified several issues with the output. We can now present this critique as feedback to the original generator so it can amend its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2677,
     "status": "ok",
     "timestamp": 1753877322546,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "wjwc0T5TQydT"
   },
   "outputs": [],
   "source": [
    "second_response = client_groq.chat.completions.create(\n",
    "    model=MAIN_MODEL,\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': medical_note_data},\n",
    "        {'role': 'assistant', 'content': first_response},\n",
    "        {'role': 'user', 'content': first_critique}\n",
    "    ],\n",
    "    temperature=0.2\n",
    ").choices[0].message.content\n",
    "\n",
    "print(second_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI9qt1WBQ5Iq"
   },
   "source": [
    "As we can see from the output above, the critique can be used to improve the response over a series of reflective interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stmkE0wSRhGR"
   },
   "source": [
    "# Pattern 2: Tool Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S62TeGujSZEd"
   },
   "source": [
    "Let us see how tool use can augement LLM capabilities with a simple example. First, we beign by defining a series of Python functions that we then wrap as LangChain tools using the `@tool` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1753877571546,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "9gjLgyeQRY7f"
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract b from a\n",
    "\n",
    "    Args:\n",
    "        a: bigger int\n",
    "        b: smaller int\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evmt8aKZni_K"
   },
   "source": [
    "Note that the doc strings of the function describing what the functions do is a critical input parsed by the tool to understand which function needs to be called when a user input is received.\n",
    "\n",
    "We then collect these tools into a dictionary with function names as the keys like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1753877573834,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "D2g5VeHLSee8"
   },
   "outputs": [],
   "source": [
    "available_tools = {'add': add, 'multiply': multiply, 'subtract': subtract}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zxg9RdYJn3-7"
   },
   "source": [
    "We can now create a basic agent by binding the LLM with these three tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1753877637632,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "wJkuPHe1ShC1"
   },
   "outputs": [],
   "source": [
    "client_groq_3 = ChatGroq(api_key=groq_api_key, model='openai/gpt-oss-20b')\n",
    "agent_2 = client_groq_3.bind_tools(list(available_tools.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuBum5Y_oDvp"
   },
   "source": [
    "Now our agent is capable of answering questions that can be resolved as evaluations of the three functions available to it as tools. Consider the following user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1753877727815,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "krKehYyJSiSd"
   },
   "outputs": [],
   "source": [
    "system_message = SystemMessage(\"Answer the question using the available tools only.\")\n",
    "query = HumanMessage(\"(3 * 12) > (11 + 49) ? what is absolute difference between the two ?\")\n",
    "\n",
    "messages=[system_message, query]\n",
    "agent_2.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2.invoke(messages).tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check agent tool registration and show available tools\n",
    "print('Agent tools:', agent_2.tools if hasattr(agent_2, 'tools') else 'No tools found')\n",
    "print('Agent config:', agent_2)\n",
    "\n",
    "# Correct invocation using LangChain message objects\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"Answer the question using the available tools only.\")\n",
    "user_message = HumanMessage(content=\"(3 * 12) > (11 + 49) ? what is absolute difference between the two ?\")\n",
    "messages = [system_message, user_message]\n",
    "\n",
    "response = agent_2.invoke(messages)\n",
    "print('Agent response:', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akrAw53HTCNd"
   },
   "source": [
    "Notice how the LLM behavior changed. Instead of answering the question correctly, it has composed a tool call output that is an intermediate step to answer the question from the user.\n",
    "\n",
    "Specifically, it has correctly recognized that it has to call the functions `multiply` and `add` with the correct arguments in order to answer the user question.\n",
    "\n",
    "Note that this is still a partial execution of a tool-calling agent. We will see an end-to-end execution of the tool-calling agent in a future session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsTArmBTTOwi"
   },
   "source": [
    "# Pattern 3: Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Tv-RbyZok7h"
   },
   "source": [
    "Planning agents utilize a specified algorithm to plan/structure their efforts to achieve a business objective. Let us see an example of a Reasoning and Action (ReAct) agent. We will take a much deeper look into ReAct agents in upcoming sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 888,
     "status": "ok",
     "timestamp": 1753877882116,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "ewZTbUVfSpsT",
    "outputId": "94073cb1-7882-40d2-f3f5-267806bfd994"
   },
   "outputs": [],
   "source": [
    "react_prompt = hub.pull(\"hwchase17/react\")\n",
    "print(react_prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atLZm8Jho0pM"
   },
   "source": [
    "As the above prompt template indicates, the LLM is asked to 'think' through before answering in a Thought/Action/Observation sequence till a final answer is reached.\n",
    "With this prompt, let us now create a simple Python agent that will always use the Python interpretor to answer user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1753878168318,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "z3e1IHGjUpGE"
   },
   "outputs": [],
   "source": [
    "python_repl = PythonREPL()\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell used to execute python commands. Input should be a valid python command.\",\n",
    "    func=python_repl.run,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEXS8KrxpHZi"
   },
   "source": [
    "As we have seen in previous examples, the `repl_tool` is basically a wrapper around the `python_repl` function. We can now create the ReAct agent by binding this `repl_tool` to the LLM like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1753878172468,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "uidHT3ZqUBYz"
   },
   "outputs": [],
   "source": [
    "client_groq = ChatGroq(api_key=groq_api_key, model='gemma2-9b-it')\n",
    "react_agent = create_react_agent(llm=client_groq, tools=[repl_tool], prompt=react_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWbTK1ScpYuP"
   },
   "source": [
    "In `LangChain` agent execution is handled by executors that track the tool calls and execute them in dedicated threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1753878174812,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "Hhnw-B19VY2d"
   },
   "outputs": [],
   "source": [
    "react_agent_executor = AgentExecutor(agent=react_agent, tools=[repl_tool], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtjYRzgoph16"
   },
   "source": [
    "Let us now test our Python tool-calling agent with a non-trivial math problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1753878219160,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "2deXuXADVEdd"
   },
   "outputs": [],
   "source": [
    "# user_input = \"If USD 450 amounts to USD 630 in 6 years, what will it amount to in 2 years at the same interest rate?\"\n",
    "user_input = \"What is the smallest number that can be represented as the sum of 2 different cubes in 2 different ways? What is the next smallest number and what are its cube numbers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = react_agent_executor.invoke({'input': user_input})\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"If USD 450 amounts to USD 630 in 6 years, what will it amount to in 2 years at the same interest rate?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = react_agent_executor.invoke({'input': user_input})\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLIbm6SSVuT3"
   },
   "source": [
    "# Pattern 4: Multi-agent Collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW_6BH20p11F"
   },
   "source": [
    "In one pattern of multi-agent collaboration called the `Triage` mode, a focal agent is tasked with handing-off tasks to appropriate agents. As an example, consider the following scenario where there are two agents - A & B. Agent A is the main agent that has two tools - a function to greet customers and to transfer control to Agent B. Agent B can only speak in Hindi, but can only be reached when Agent A hands-off control.\n",
    "\n",
    "Whether the control needs to reach Agent B or not is decided by Agent A depending on the user query.\n",
    "\n",
    "We will look at many more patterns of multi-agent collaborations in an upcoming session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm_client_groq = Swarm(client_groq_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1753878532935,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "rU4RqFhpWN5D"
   },
   "outputs": [],
   "source": [
    "def transfer_to_agent_b():\n",
    "    return agent_b\n",
    "\n",
    "\n",
    "def transfer_to_agent_a():\n",
    "    return agent_a\n",
    "\n",
    "\n",
    "def greet_customer():\n",
    "    return \"Hello, how can I help you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 129,
     "status": "ok",
     "timestamp": 1753878539039,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "RyKNxIyTVQV9"
   },
   "outputs": [],
   "source": [
    "agent_a = Agent(\n",
    "    name=\"Agent A\",\n",
    "    instructions=\"You are a helpful agent.\", \n",
    "    model='gemma2-9b-it', \n",
    "    functions=[transfer_to_agent_b, greet_customer]\n",
    ")\n",
    "\n",
    "agent_b = Agent(\n",
    "    name=\"Agent B\", \n",
    "    instructions=\"Only speak in Hindi.\", \n",
    "    model='gemma2-9b-it', \n",
    "    functions=[transfer_to_agent_a]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1644,
     "status": "ok",
     "timestamp": 1753878709941,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "tWY0mT9RWYDC"
   },
   "outputs": [],
   "source": [
    "response = swarm_client_groq.run(\n",
    "    agent=agent_a, \n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    ")\n",
    "print(response.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1753878715252,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "DRyR1aBYqZhT",
    "outputId": "75126fbb-8ecb-4d7f-e5b7-7afa21c6f8e3"
   },
   "outputs": [],
   "source": [
    "len(response.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1753878716760,
     "user": {
      "displayName": "ankur saxena",
      "userId": "11041009981365514655"
     },
     "user_tz": -330
    },
    "id": "DjuUudHIsZ0R",
    "outputId": "be5b2e03-a9dc-436b-b7d2-739487e03d83"
   },
   "outputs": [],
   "source": [
    "for message in response.messages:\n",
    "    print(message+'\\n----')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
